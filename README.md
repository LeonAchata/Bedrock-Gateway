# LLM Gateway (MCP Server)

**Gateway centralizado para mÃºltiples modelos de AWS Bedrock a travÃ©s del Model Context Protocol (MCP)**

Este proyecto implementa un servidor MCP que permite a agentes y workflows de IA comunicarse con **15+ modelos foundation de AWS Bedrock** (Nova, Claude, Llama, Mistral, etc.) a travÃ©s de una interfaz MCP estandarizada.

## ğŸ¯ PropÃ³sito

El LLM Gateway actÃºa como **puente universal** entre workflows externos de IA y los modelos foundation de Bedrock, proporcionando:

- **Acceso unificado** a 15+ modelos de Bedrock (Nova, Claude, Llama, Mistral)
- **Cada agente elige su modelo** segÃºn sus necesidades (hardcodeado en el agente)
- **Caching inteligente** de respuestas para reducir costos y latencia
- **MÃ©tricas detalladas** de uso, costos y rendimiento
- **Interfaz MCP estÃ¡ndar** para conexiÃ³n universal de agentes

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent A     â”‚       â”‚  Agent B     â”‚       â”‚  Agent C     â”‚
â”‚  (nova-pro)  â”‚       â”‚  (claude)    â”‚       â”‚  (llama)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚                       â”‚
       â”‚        MCP Protocol (stdio/SSE)              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   LLM Gateway      â”‚
                   â”‚   (MCP Server)     â”‚
                   â”‚                    â”‚
                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                   â”‚  â”‚   Router     â”‚  â”‚
                   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
                   â”‚  â”‚   Cache      â”‚  â”‚
                   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
                   â”‚  â”‚   Metrics    â”‚  â”‚
                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
              â–¼              â–¼              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Nova   â”‚    â”‚ Claude  â”‚   â”‚  Llama   â”‚
        â”‚  Models â”‚    â”‚ Models  â”‚   â”‚  Models  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              AWS Bedrock Foundation Models
```

## ğŸ“ Estructura del Proyecto

```
llm-gateway/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.py           # Servidor FastMCP principal
â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n (solo AWS Bedrock)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/             # ğŸ†• CatÃ¡logo de modelos Bedrock
â”‚   â”‚   â”œâ”€â”€ bedrock_models.py  # 15+ FMs con pricing
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ bedrock/            # ğŸ†• Cliente Bedrock universal
â”‚   â”‚   â”œâ”€â”€ bedrock_client.py  # Cliente Ãºnico para todos los modelos
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ mcp/                # MCP Tools
â”‚   â”‚   â”œâ”€â”€ tools.py        # generate, list_models, get_stats
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/               # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ router.py       # Enrutamiento a modelos
â”‚   â”‚   â”œâ”€â”€ cache.py        # Sistema de cachÃ©
â”‚   â”‚   â”œâ”€â”€ metrics.py      # Tracking de mÃ©tricas
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utilidades
â”‚       â”œâ”€â”€ logger.py       # Logging centralizado
â”‚       â”œâ”€â”€ validators.py   # Validaciones
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ParaAgente/             # ğŸ¯ IntegraciÃ³n para agentes LangGraph
â”‚   â”œâ”€â”€ bedrock_client.py   # Cliente MCP (stdio)
â”‚   â”œâ”€â”€ llm_node.py         # Nodo reutilizable LangGraph
â”‚   â”œâ”€â”€ example_agent.py    # 5 ejemplos completos
â”‚   â”œâ”€â”€ requirements.txt    # Dependencias del agente
â”‚   â””â”€â”€ README.md           # GuÃ­a de integraciÃ³n
â”‚
â”œâ”€â”€ Dockerfile              # ğŸ³ Imagen Docker para producciÃ³n
â”œâ”€â”€ docker-compose.yml      # Despliegue fÃ¡cil con Docker Compose
â”œâ”€â”€ .dockerignore           # Exclusiones de build
â”œâ”€â”€ DOCKER_DEPLOYMENT.md    # ğŸ“– GuÃ­a completa de Docker
â”‚
â”œâ”€â”€ requirements.txt        # Dependencias del gateway
â”œâ”€â”€ .env.example            # Template de variables de entorno
â”œâ”€â”€ mcp_config.example.json # ConfiguraciÃ³n MCP de ejemplo
â”œâ”€â”€ AGENT_EXAMPLES.md       # Ejemplos de configuraciÃ³n de agentes
â””â”€â”€ README.md               # Este archivo
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ mcp_config.example.json
â”œâ”€â”€ AGENT_EXAMPLES.md       # ğŸ†• Ejemplos de configuraciÃ³n
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar e instalar dependencias

```bash
cd llm-gateway
pip install -r requirements.txt
```

### 2. Configurar variables de entorno

Crear archivo `.env` con tus credenciales AWS:

```bash
# AWS Bedrock (Ãºnica configuraciÃ³n necesaria)
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1

# Cache y mÃ©tricas
CACHE_ENABLED=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000
METRICS_ENABLED=true

# Logging
LOG_LEVEL=INFO
```

### 3. Ejecutar el servidor

```bash
python -m src.server
```

## ğŸ“‹ Modelos Disponibles

El gateway soporta **15+ modelos de Bedrock**:

### Amazon Nova
- `nova-pro` - Avanzado, razonamiento superior ($0.0008/$0.0032 per 1K)
- `nova-lite` - RÃ¡pido y econÃ³mico ($0.00006/$0.00024 per 1K)
- `nova-micro` - Ultra rÃ¡pido, bÃ¡sico ($0.000035/$0.00014 per 1K)

### Anthropic Claude
- `claude-3-5-sonnet` - MÃ¡s inteligente ($0.003/$0.015 per 1K)
- `claude-3-5-haiku` - MÃ¡s rÃ¡pido ($0.001/$0.005 per 1K)
- `claude-3-opus` - MÃ¡s poderoso ($0.015/$0.075 per 1K)
- `claude-3-sonnet` - Balanceado ($0.003/$0.015 per 1K)
- `claude-3-haiku` - Eficiente ($0.00025/$0.00125 per 1K)

### Meta Llama
- `llama-3-3-70b` - Ãšltimo modelo 70B ($0.00065/$0.00065 per 1K)
- `llama-3-2-90b` - Multimodal con visiÃ³n ($0.0008/$0.0008 per 1K)
- `llama-3-2-11b` - PequeÃ±o multimodal ($0.00016/$0.00016 per 1K)
- `llama-3-1-70b` - 70B parÃ¡metros ($0.00099/$0.00099 per 1K)
- `llama-3-1-8b` - PequeÃ±o y eficiente ($0.00022/$0.00022 per 1K)

### Mistral
- `mistral-large-2` - Flagship, razonamiento avanzado ($0.003/$0.009 per 1K)
- `mistral-small` - RÃ¡pido y econÃ³mico ($0.001/$0.003 per 1K)

**Ver lista completa:** `await mcp.call_tool("list_models", {})`

## ğŸ”Œ ConexiÃ³n desde Agentes

### Concepto Clave

**Cada agente especifica su modelo en el cÃ³digo**, no en la configuraciÃ³n. El gateway es un Ãºnico punto de entrada para todos los modelos de Bedrock.

### ConfiguraciÃ³n MCP (igual para todos los agentes)

```json
{
  "mcpServers": {
    "llm-gateway": {
      "command": "python",
      "args": ["-m", "src.server"],
      "env": {
        "AWS_REGION": "us-east-1",
        "AWS_ACCESS_KEY_ID": "your-key",
        "AWS_SECRET_ACCESS_KEY": "your-secret"
      }
    }
  }
}
```

### Agent A - Usa Nova Pro

```python
# Este agente siempre usa nova-pro
response = await session.call_tool(
    "generate",
    {
        "model": "nova-pro",  # <-- Modelo hardcodeado en el agente
        "messages": [{"role": "user", "content": "Tarea compleja..."}],
        "temperature": 0.7,
        "max_tokens": 2000
    }
)
```

### Agent B - Usa Claude 3.5 Sonnet

```python
# Este agente siempre usa claude-3-5-sonnet
response = await session.call_tool(
    "generate",
    {
        "model": "claude-3-5-sonnet",  # <-- Modelo diferente
        "messages": [{"role": "user", "content": "AnÃ¡lisis profundo..."}]
    }
)
```

### Agent C - Usa Llama 3.3 70B

```python
# Este agente siempre usa llama-3-3-70b
response = await session.call_tool(
    "generate",
    {
        "model": "llama-3-3-70b",  # <-- Otro modelo
        "messages": [{"role": "user", "content": "Tarea general..."}]
    }
)
```

**Ver mÃ¡s ejemplos:** [AGENT_EXAMPLES.md](AGENT_EXAMPLES.md)

### Ejemplo completo desde un agente (Python)

```python
from mcp import ClientSession
from mcp.client.stdio import stdio_client

async def use_llm_gateway():
    # Conectar al gateway
    async with stdio_client("python", ["-m", "src.server"]) as (read, write):
        async with ClientSession(read, write) as session:
            # Inicializar
            await session.initialize()
            
            # Listar modelos disponibles
            models = await session.call_tool("list_models", {})
            print(f"Available: {len(models)} models")
            
            # Generar completion con el modelo especÃ­fico del agente
            response = await session.call_tool(
                "generate",
                {
                    "model": "nova-pro",  # <-- El agente elige su modelo
                    "messages": [
                        {"role": "user", "content": "Â¿QuÃ© es IA?"}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 500
                }
            )
            
            print(f"Response: {response['content']}")
            print(f"Tokens: {response['usage']['total_tokens']}")
            print(f"Cost: ${response['estimated_cost_usd']:.6f}")
            print(f"Cached: {response['cached']}")
            print(f"Latency: {response['latency_ms']:.2f}ms")
```

## ğŸ› ï¸ Herramientas MCP Disponibles

### 1. `generate`

Genera completions usando cualquier modelo de Bedrock.

**ParÃ¡metros:**
- `model` (str): Nombre corto del modelo (ej: "nova-pro", "claude-3-5-sonnet", "llama-3-3-70b")
- `messages` (list): Lista de mensajes con 'role' y 'content'
- `temperature` (float): Temperatura de muestreo (0.0-2.0)
- `max_tokens` (int): MÃ¡ximo de tokens a generar

**Retorna:**
```json
{
  "content": "Respuesta generada...",
  "model": "nova-pro",
  "model_id": "us.amazon.nova-pro-v1:0",
  "usage": {
    "input_tokens": 10,
    "output_tokens": 50,
    "total_tokens": 60
  },
  "finish_reason": "stop",
  "cached": false,
  "latency_ms": 1234.56,
  "estimated_cost_usd": 0.001234
}
```

### 2. `list_models`

Lista todos los modelos de Bedrock disponibles con pricing.

**Retorna:**
```json
[
  {
    "name": "nova-pro",
    "model_id": "us.amazon.nova-pro-v1:0",
    "description": "Advanced multimodal AI model with superior reasoning",
    "context_window": 300000,
    "input_cost_per_1k": 0.0008,
    "output_cost_per_1k": 0.0032,
    "supports_system": true,
    "max_tokens": 5000
  },
  ...
]
```

### 3. `get_stats`

Obtiene estadÃ­sticas del gateway (mÃ©tricas y cachÃ©).

**Retorna:**
```json
{
  "metrics": {
    "total_requests": 100,
    "total_tokens": 50000,
    "total_cost_usd": 1.23,
    "cache_hit_rate_percent": 45.5,
    "average_latency_ms": 1234.5,
    "requests_by_model": {
      "nova-pro": 50,
      "claude-3-5-sonnet": 30,
      "llama-3-3-70b": 20
    }
  },
  "cache": {
    "current_size": 50,
    "max_size": 1000,
    "enabled": true
  }
}
```

## ğŸ“Š CaracterÃ­sticas

### âœ… Implementadas

- âœ… Servidor FastMCP con protocolo estÃ¡ndar
- âœ… **15+ modelos de Bedrock** (Nova, Claude, Llama, Mistral)
- âœ… **Cliente Bedrock universal** - un solo cliente para todos los modelos
- âœ… **Cada agente elige su modelo** - hardcodeado en el cÃ³digo del agente
- âœ… Sistema de cachÃ© con TTL
- âœ… MÃ©tricas detalladas por modelo (requests, tokens, costos, latencias)
- âœ… Validaciones centralizadas
- âœ… Logging estructurado
- âœ… EstimaciÃ³n de costos automÃ¡tica con pricing real

### ğŸ”® Futuras

- ğŸ”œ Rate limiting por agente
- ğŸ”œ Persistencia de mÃ©tricas (base de datos)
- ğŸ”œ Dashboard web de monitoreo
- ğŸ”œ Streaming de respuestas
- ğŸ”œ Soporte para modelos con imÃ¡genes (multimodales)

## ğŸ“ Variables de Entorno

| Variable | DescripciÃ³n | Default |
|----------|-------------|---------|
| `AWS_ACCESS_KEY_ID` | AWS Access Key | - |
| `AWS_SECRET_ACCESS_KEY` | AWS Secret Key | - |
| `AWS_REGION` | AWS Region | us-east-1 |
| `CACHE_ENABLED` | Habilitar cachÃ© | true |
| `CACHE_TTL` | TTL del cachÃ© (segundos) | 3600 |
| `CACHE_MAX_SIZE` | TamaÃ±o mÃ¡ximo del cachÃ© | 1000 |
| `METRICS_ENABLED` | Habilitar mÃ©tricas | true |
| `LOG_LEVEL` | Nivel de logging | INFO |

## â“ FAQ

### Â¿Por quÃ© solo Bedrock y no otros proveedores?

Este gateway estÃ¡ optimizado para **entornos empresariales** donde Bedrock ofrece:
- 15+ modelos bajo una sola infraestructura
- Seguridad y compliance empresarial
- Sin lÃ­mites de rate por usuario (lÃ­mites por cuenta AWS)
- Pricing predecible y sin cargos ocultos

### Â¿CÃ³mo decide cada agente quÃ© modelo usar?

El modelo se especifica **en el cÃ³digo del agente**, no en configuraciÃ³n:

```python
# Agente A
response = await mcp.call_tool("generate", {"model": "nova-pro", ...})

# Agente B  
response = await mcp.call_tool("generate", {"model": "claude-3-5-sonnet", ...})

# Agente C
response = await mcp.call_tool("generate", {"model": "llama-3-3-70b", ...})
```

### Â¿Puedo tener mÃºltiples agentes conectados al mismo gateway?

**SÃ­**, es el caso de uso principal. Todos los agentes se conectan al mismo gateway MCP, pero cada uno especifica su modelo preferido. El gateway:
- Cachea respuestas compartidas entre agentes
- Trackea mÃ©tricas por modelo
- Optimiza costos con cachÃ© inteligente

### Â¿CÃ³mo agrego un nuevo modelo de Bedrock?

Edita `src/models/bedrock_models.py` y agrega el modelo al diccionario `BEDROCK_MODELS`. Ejemplo:

```python
"mi-modelo": BedrockModel(
    model_id="aws.mi-modelo-v1:0",
    name="Mi Modelo Nuevo",
    description="DescripciÃ³n",
    context_window=128000,
    input_cost_per_1k=0.001,
    output_cost_per_1k=0.002,
    supports_system=True,
    max_tokens=4096
)
```

## ğŸ³ Despliegue con Docker

### Quick Start Local

```bash
# 1. Configurar variables de entorno
cp .env.example .env
# Editar .env con tus AWS credentials

# 2. Build y run con Docker Compose
docker-compose up -d

# 3. Ver logs
docker-compose logs -f
```

### Build Manual

```bash
# Build de la imagen
docker build -t bedrock-gateway:latest .

# Run con variables de entorno
docker run -d \
  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  -e AWS_REGION=us-east-1 \
  -e CACHE_ENABLED=true \
  bedrock-gateway:latest
```

### CaracterÃ­sticas Docker

- âœ… Multi-stage build (imagen optimizada ~150MB)
- âœ… Usuario no-root (seguridad)
- âœ… VolÃºmenes persistentes para logs
- âœ… Health checks configurables
- âœ… Resource limits (CPU/memoria)
- âœ… Compatible con Docker Compose y Kubernetes

**ğŸ“– GuÃ­a completa:** Ver [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md) para:
- ConfiguraciÃ³n avanzada
- Escalado horizontal
- Monitoreo y debugging
- Despliegue en producciÃ³n (ECS/Fargate)
- Troubleshooting completo

## ğŸ¯ IntegraciÃ³n con Agentes LangGraph

### Quick Start

La carpeta `ParaAgente/` contiene todo lo necesario:

```bash
# 1. Instalar dependencias del agente
cd ParaAgente
pip install -r requirements.txt

# 2. Usar el nodo LLM en tu grafo
from llm_node import AgentState, llm_consultation_node
from langgraph.graph import StateGraph, END

workflow = StateGraph(AgentState)
workflow.add_node("llm", llm_consultation_node)
workflow.set_entry_point("llm")
workflow.add_edge("llm", END)

app = workflow.compile()

# 3. Ejecutar
result = await app.ainvoke({
    "messages": [{"role": "user", "content": "Hola"}],
    "model": "nova-pro"
})
print(result["response"])
```

### Contenido de ParaAgente/

- **`bedrock_client.py`**: Cliente MCP (stdio) para el gateway
- **`llm_node.py`**: Nodo reutilizable de LangGraph
- **`example_agent.py`**: 5 ejemplos completos
- **`README.md`**: GuÃ­a de integraciÃ³n detallada

**ğŸ“– DocumentaciÃ³n completa:** Ver [ParaAgente/README.md](ParaAgente/README.md)

## ğŸ†˜ Soporte

Para problemas o preguntas:
1. Ver ejemplos en [AGENT_EXAMPLES.md](AGENT_EXAMPLES.md)
2. Ver integraciÃ³n con LangGraph en [ParaAgente/README.md](ParaAgente/README.md)
3. Ver despliegue Docker en [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)
4. Revisar logs con `LOG_LEVEL=DEBUG`
5. Verificar credenciales AWS en `.env`
6. Crear un issue en el repositorio

---

**Nota**: Este es un servidor MCP puro. No expone endpoints REST. Los agentes deben conectarse usando el protocolo MCP (stdio o SSE).
